<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ctownsen357@github.io</title>
    <link>http://ctownsen357.github.io/tags/development/index.xml</link>
    <description>Recent content on ctownsen357@github.io</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <atom:link href="http://ctownsen357.github.io/tags/development/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>r and ff, and many columns! Oh my!</title>
      <link>http://ctownsen357.github.io/posts/r-ff-many-columns/</link>
      <pubDate>Fri, 02 Dec 2016 10:28:40 -0500</pubDate>
      
      <guid>http://ctownsen357.github.io/posts/r-ff-many-columns/</guid>
      <description>

&lt;p&gt;Recently I was experimenting with a data set in R that ended up being more challenging than I first expected.  It really wasn&amp;rsquo;t that large as far as size on disk or memory(single instance) was concerned but it had over 3,000 columns.&lt;/p&gt;

&lt;p&gt;At first I didn&amp;rsquo;t think it was that big of a deal and it wouldn&amp;rsquo;t be if one only needed a single instance of the ~ 750MB file in memory.  What if you have a web application that instantiates that data set on a server for each user and you have many concurrent users? All of a sudden that 750MB and any operations on that data can quickly exhaust available RAM.&lt;/p&gt;

&lt;p&gt;I looked at the bigmemory package because it looked promising and contained some tech under the hood that I&amp;rsquo;ve used: memory mapped files and shared memory via the C++ boost libraries.  That combination allows a single instance of a matrix to be accessed by multiple processes.  The problem with that ended up being that the dat set was not a matrix of uniform type but mixed types.  Enter the ff and ffbase packages!&lt;/p&gt;

&lt;p&gt;The ff package provides data structures that are stored on disk but behave (almost) as if they were in RAM by transparently mapping only a section (pagesize) in main memory - the effective virtual memory consumption per ff object.&lt;/p&gt;

&lt;p&gt;I loaded the original data frame and tried to cast it to an ffdf (ff data frame):&lt;/p&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span style=&#34;color: #f92672&#34;&gt;require&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;(&lt;/span&gt;ffbase&lt;span style=&#34;color: #f8f8f2&#34;&gt;)&lt;/span&gt;
&lt;span style=&#34;color: #66d9ef&#34;&gt;load&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color: #e6db74&#34;&gt;&amp;#39;./35509-0001-Data.rda&amp;#39;&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;)&lt;/span&gt;
ffdf35509.0001 &lt;span style=&#34;color: #f92672&#34;&gt;&amp;lt;-&lt;/span&gt; as.ffdf&lt;span style=&#34;color: #f8f8f2&#34;&gt;(&lt;/span&gt;da35509.0001&lt;span style=&#34;color: #f8f8f2&#34;&gt;,&lt;/span&gt; vmode &lt;span style=&#34;color: #f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color: #66d9ef&#34;&gt;NULL&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;)&lt;/span&gt;
save.ffdf&lt;span style=&#34;color: #f8f8f2&#34;&gt;(&lt;/span&gt;fromdf&lt;span style=&#34;color: #f8f8f2&#34;&gt;,&lt;/span&gt; dir&lt;span style=&#34;color: #f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #e6db74&#34;&gt;&amp;quot;./testdf&amp;quot;&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;,&lt;/span&gt; relative&lt;span style=&#34;color: #f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color: #66d9ef&#34;&gt;TRUE&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It failed silently.  I looked in the &lt;code&gt;./testdf&lt;/code&gt; and nothing.  Ugh, what appeared so promising now looked bleak!&lt;/p&gt;

&lt;p&gt;After some additional tinkering, I received an actual error.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error en  ff(initdata = initdata, length = length, levels = levels, ordered = ordered,  : 
   write error

&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Not very descriptive but it was something to go on.  After some searching &lt;a href=&#34;http://stackoverflow.com/questions/14025202/ff-package-write-error&#34;&gt;I came across this thread on stackoverflow.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The answers were not entirely helpful but they did discuss that each column in an ff data frame are stored in distinct files.  Bingo! I&amp;rsquo;ve encountered bumping into exceeding the max number of files open on a system before and it is a very simple fix.  Keep in mind the file limits are in place to prevent resource exhaustion on the system.  Those limits are set VERY conservatively though.  Increasing the limits is as simple as the following:&lt;/p&gt;

&lt;h3 id=&#34;linux&#34;&gt;Linux:&lt;/h3&gt;

&lt;p&gt;Add the following to your &lt;code&gt;/etc/security/limits.conf&lt;/code&gt; file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;youruserid  hard  nofile 100000 # you may enter whatever number you wish here
youruserid  soft  nofile  10000 # whatever you want the default to be for each shell or process you have running
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;os-x&#34;&gt;OS X:&lt;/h3&gt;

&lt;p&gt;Add or edit the following in your &lt;code&gt;/etc/sysctl.con&lt;/code&gt; file.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kern.maxfilesperproc=166384
kern.maxfiles=8192
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You&amp;rsquo;ll need to log out and log back in for the change to take effect.  After that one my use the ff data frame and the 750MB data frame with over 3,000 columns only consumes ~ 11MB of RAM per instance.  Now, many parallel instances of the R routines using this data may be run without exhausting available RAM.  Keep in mind that you&amp;rsquo;ll want to tweak your max open file settings to account for expected concurrent use.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GlusterFS &amp; Docker</title>
      <link>http://ctownsen357.github.io/posts/glusterfs-docker/</link>
      <pubDate>Tue, 08 Nov 2016 16:28:48 -0500</pubDate>
      
      <guid>http://ctownsen357.github.io/posts/glusterfs-docker/</guid>
      <description>

&lt;p&gt;I&amp;rsquo;ve recently started exploring &lt;a href=&#34;https://www.gluster.org/&#34;&gt;GlusterFS&lt;/a&gt; in Docker containers to use as persistent storage for the Dockerized services and applications I&amp;rsquo;ve been working on.  If this is performant enough then for my purposes it will close the gap for me to really treat the data center as one giant computer.  Getting started was pretty straight forward.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; This is a quick example.  Make sure you read up on security, changing the default password, and review the original Dockerfile.  I&amp;rsquo;ll be experimenting with running this out on AWS soon and should be able to further tighten up my example.&lt;/p&gt;

&lt;h3 id=&#34;get-the-latest-gluster-container&#34;&gt;Get the latest Gluster container:&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://github.com/gluster/gluster-containers&#34;&gt;Get the latest container:&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;docker pull gluster/gluster-centos
&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&#34;make-the-persistent-data-folders-on-each-host&#34;&gt;Make the persistent data folders on each host:&lt;/h3&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;sudo mkdir -p /gluster/logs &lt;span style=&#34;color: #f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; sudo mkdir /gluster/data &lt;span style=&#34;color: #f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; sudo mkdir /gluster/config &lt;span style=&#34;color: #f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; /gluster/mnt
&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&#34;start-a-glusterfs-container-on-each-host&#34;&gt;Start a GlusterFS container on each host:&lt;/h3&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;docker run -d &lt;span style=&#34;color: #ae81ff&#34;&gt;\&lt;/span&gt;
   --name gluster &lt;span style=&#34;color: #ae81ff&#34;&gt;\&lt;/span&gt;
   --privileged &lt;span style=&#34;color: #ae81ff&#34;&gt;\&lt;/span&gt;
   --net&lt;span style=&#34;color: #f92672&#34;&gt;=&lt;/span&gt;host &lt;span style=&#34;color: #ae81ff&#34;&gt;\&lt;/span&gt;
   -v /gluster/data:/gluster &lt;span style=&#34;color: #ae81ff&#34;&gt;\&lt;/span&gt;
   -v /gluster/logs:/var/log/glusterfs &lt;span style=&#34;color: #ae81ff&#34;&gt;\&lt;/span&gt;
   -v /gluster/config:/var/lib/glusterd &lt;span style=&#34;color: #ae81ff&#34;&gt;\&lt;/span&gt;
   -v /gluster/mnt:/gluster/mnt &lt;span style=&#34;color: #ae81ff&#34;&gt;\&lt;/span&gt;
   gluster/gluster-centos
&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&#34;probe-the-hosts-in-the-cluster&#34;&gt;Probe the hosts in the cluster:&lt;/h3&gt;

&lt;p&gt;For each container on each host you&amp;rsquo;ll want to execute this to get them aware of the other peers.  If running out on AWS these steps could be orchestrated through the init system on the hosts so you don&amp;rsquo;t have to log into each machine.&lt;/p&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;gluster peer probe 1.1.1.1
&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&#34;now-create-your-volume-and-start-it&#34;&gt;Now create your volume and start it:&lt;/h3&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;gluster volume create media replica &lt;span style=&#34;color: #ae81ff&#34;&gt;3&lt;/span&gt; transport tcp 172.30.0.185:/gluster/data  172.30.0.186:/gluster/data 172.30.0.30:/
gluster volume start media
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this example I&amp;rsquo;m replicating across each of three servers but depending on your needs you could: distributed striped, distributed, replicated, distributed striped replicated, &amp;hellip; know what and why.&lt;/p&gt;

&lt;h3 id=&#34;mount-the-volume&#34;&gt;Mount the volume&lt;/h3&gt;

&lt;p&gt;The docs made a big deal out of mounting the volume.  I suspect if you were doing anything other than replicating that would become very important.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;ll want to do this on each host, using its internal ip:&lt;/p&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;mount -t glusterfs 172.30.0.186:/media /gluster/mnt
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;From one of the hosts testing with a write statement to the volume:&lt;/p&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;echo&lt;/span&gt; &lt;span style=&#34;color: #e6db74&#34;&gt;&amp;quot;testing, 1,2,3...&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; /gluster/mnt/test.txt
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;And from another host you should be able to read/write to the same document.  One could then launch containers on any host with a mount to /gluster/mnt to store data.  Then it would have access to the data no matter which node it was launched on.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Manual CoreOS Update</title>
      <link>http://ctownsen357.github.io/posts/manual-coreos-update/</link>
      <pubDate>Thu, 03 Nov 2016 17:00:49 -0400</pubDate>
      
      <guid>http://ctownsen357.github.io/posts/manual-coreos-update/</guid>
      <description>&lt;p&gt;This forces a manual update if one is available:&lt;/p&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;update_engine_client -update
&lt;/pre&gt;&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Argument List Too Long</title>
      <link>http://ctownsen357.github.io/posts/argument-list-too-log/</link>
      <pubDate>Tue, 01 Nov 2016 15:47:38 -0400</pubDate>
      
      <guid>http://ctownsen357.github.io/posts/argument-list-too-log/</guid>
      <description>&lt;p&gt;You know it before you even try.  You try anyway.  Something like trying to bzip2  316,387 CSV files representing ~ 10TB of data.  You know it will be at least &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;10&lt;/sub&gt; the size and R can handle the bzip2 files directly so you call bzip2 and get the argument list too long error.&lt;/p&gt;

&lt;p&gt;Find and xargs to the rescue!  Also, lbzip2 because you really don&amp;rsquo;t want to wait on bzip2 any more than you have to.&lt;/p&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;find path_to_files/ -name &lt;span style=&#34;color: #e6db74&#34;&gt;&amp;quot;*.csv&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #f8f8f2&#34;&gt;|&lt;/span&gt; xargs -P &lt;span style=&#34;color: #ae81ff&#34;&gt;5&lt;/span&gt; lbzip2
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; you&amp;rsquo;ll want to balance the number of parallel lbzip2 instances (or whatever you are running) as indicated by -P 5 above, balance that with the fact that lbzip2 runs in parallel too.&lt;/p&gt;

&lt;p&gt;It still took a long time to compress 316,387 files but it went a whole lot faster with parallel instances of parallel bzip2 using the xargs -P flag.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CoreOS Mount NTFS Share</title>
      <link>http://ctownsen357.github.io/posts/coreos-mount-ntfs-share/</link>
      <pubDate>Tue, 01 Nov 2016 14:12:12 -0400</pubDate>
      
      <guid>http://ctownsen357.github.io/posts/coreos-mount-ntfs-share/</guid>
      <description>&lt;p&gt;I had the need to mount an NTFS share for an application that was connecting to a SQL Server database and required that a share be mapped.  While testing from my CentOS 7 desktop, creating the share was trivial.  Not so much once I transitioned over to a CoreOS machine where I was to deploy for user testing.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I map /opt/bin from the host to a folder in the container.  I store custom binaries and scripts in /opt/bin on CoreOS as it is in the path and persists even after CoreOS updates to the latest version.  I&amp;rsquo;ve also changed the example from a Fedora container to CentOS as CentOS already has items installed that the example I link to installed in addition to the dev tools and libraries.&lt;/p&gt;

&lt;p&gt;This is how I got around that problem:&lt;/p&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;docker run -t -i -v /opt/bin:/host_tmp centos /bin/bash
yum groupinstall -y &lt;span style=&#34;color: #e6db74&#34;&gt;&amp;quot;Development Tools&amp;quot;&lt;/span&gt; &lt;span style=&#34;color: #e6db74&#34;&gt;&amp;quot;Development Libraries&amp;quot;&lt;/span&gt;

curl https://download.samba.org/pub/linux-cifs/cifs-utils/cifs-utils-6.3.tar.bz2 &lt;span style=&#34;color: #f8f8f2&#34;&gt;|&lt;/span&gt; bunzip2 -c - &lt;span style=&#34;color: #f8f8f2&#34;&gt;|&lt;/span&gt; tar -xvf -
&lt;span style=&#34;color: #f8f8f2&#34;&gt;cd&lt;/span&gt; cifs-utils-6.3/
./configure &lt;span style=&#34;color: #f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; make
cp mount.cifs /host_tmp/
&lt;span style=&#34;color: #f8f8f2&#34;&gt;exit&lt;/span&gt;

sudo mkdir /media/foo
sudo mount.cifs &lt;span style=&#34;color: #e6db74&#34;&gt;&amp;quot;//1.1.1.1/ntfs_share&amp;quot;&lt;/span&gt; -o &lt;span style=&#34;color: #f8f8f2&#34;&gt;username&lt;/span&gt;&lt;span style=&#34;color: #f92672&#34;&gt;=&lt;/span&gt;winuser,domain&lt;span style=&#34;color: #f92672&#34;&gt;=&lt;/span&gt;mydomain.com,rw,dir_mode&lt;span style=&#34;color: #f92672&#34;&gt;=&lt;/span&gt;0775,noperm /media/foo/ 
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Originally &lt;a href=&#34;https://gist.github.com/pantelis/540a19262cacc841fb0a&#34;&gt;found here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Monitor Docker Events with Awk</title>
      <link>http://ctownsen357.github.io/posts/awk-mon/</link>
      <pubDate>Tue, 01 Nov 2016 09:38:04 -0400</pubDate>
      
      <guid>http://ctownsen357.github.io/posts/awk-mon/</guid>
      <description>&lt;p&gt;I recently wrote a Docker event monitor in Go as an excercise to demonstrate some proficiency in Go and Docker.  Before getting started I was thinking about how it could be done with piping, bash, and awk.  It was actually really easy to do.  Some of the excercise requirements were:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The service should monitor the Docker API for restart events&lt;/li&gt;
&lt;li&gt;The service should run an arbitrary command in response to that event.&lt;/li&gt;
&lt;li&gt;The arbitrary command should be supplied via a config file.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The pipe, bash, awk solution:&lt;/p&gt;
&lt;div class=&#34;highlight&#34; style=&#34;background: #272822&#34;&gt;&lt;pre style=&#34;line-height: 125%&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span style=&#34;color: #f8f8f2&#34;&gt;echo&lt;/span&gt; &lt;span style=&#34;color: #e6db74&#34;&gt;&amp;#39; pwd&amp;#39;&lt;/span&gt; &amp;gt; cmd.txt &lt;span style=&#34;color: #f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; docker events &lt;span style=&#34;color: #f8f8f2&#34;&gt;|&lt;/span&gt; awk &lt;span style=&#34;color: #e6db74&#34;&gt;&amp;#39;/container restart/{system(&amp;quot;echo docker exec &amp;quot; $4 &amp;quot; $(cat cmd.txt) | bash -&amp;quot;)}&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This pipes the pwd command into a file and then pipes the stream from docker events into awk which is searching for the restart event.  When the restart event is encountered it executes the arbitrary command from the text file against the restarted container.  The command in the text file could be replaced with any desired command.&lt;/p&gt;

&lt;p&gt;There were other requirements that made it interesting to think through in Go.  I&amp;rsquo;ll be posting the entire excercise and my code soon.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>